
## Identificação Web 

![[Pasted image 20250218133653.png]]

A fase de recon exige diversos passos para que consiga ser bem executada e com que não deixe nenhuma brecha onde possa ser um vetor de ataque, algumas das táticas que existem no web recon são:
- Parsing HTML/JS
- Mirror Website
- Robots.txt / Sitemap.xml
- Brute force Diretórios / Arquivos
- Análise de respostas HTTP
- Bypass User-Agent
- Burp Suite(proxy)
- Whatsweb
- Wappalyzer
- Análise de extensões e icon
- Análise de erros e banners 

O mapeamento da página também é algo crucial, para entender o que a página da empresa faz, os recursos que ela oferece, como ela trata os acontecimentos nela, suas funcionalidades, etc.



## Robots e Sitemap.xml

##### Robots.txt
Utilizado para impedir a indexação de uma determinada página na ferramenta de pesquisa por meio de crawlers feitos pelas própias search engines que fazem a busca e a indexação de sites e páginas web de acordo com a sua pesquisa.
Exemplo de um robots.txt:
![[Pasted image 20250218140520.png]]
-> User-agent: * --> Identificador para falar quais webcrawlers ele se refere (* = todos)
-> Disallow: --> Identifica qual os diretórios que deseja que nao faça a indexação

##### Sitemap.xml
Um arquivo que faz o mapeamento do site, mostrando quais páginas são do seu site, melhorando a indexação das search engines, etc.


## Listagem de Diretórios

É basicamente quando você consegue acessar uma pasta de diretórios de uma página (Index of / ), o que pode por vezes não apresentar riscos para a sua empresa ou apresentar graves riscos, então é algo que se tem de tomar muita atenção para não passar despercebido.
Então é uma boa prática desativar a listagem de diretórios para evitar com que a infraestrutura do site seja mapeado e navegado livremente.


## Mirror website

Espelho de site, método onde você clona a página para poder estudar ela de forma mais fácil, ou até mesmo, utilizando ela em ataques de phishing. Tal técnica abre diversos vetores de ataques.

Podemos utilizar o *wget -m(mirror)* para fazer a clonagem do site e baixar o site e construir a estrutura localmente no site.

Ás vezes o site pode não ser clonado, para isso podemos usar a opção do wget, o" *-e*"  que consegue executar comandos, então a query para ignorar o robots seria: *wget -m -e robots=off (página que deseja clonar)* 

## Análise de erros, códigos e extensões

A análise destes utensílios podem ser de extrema utilidade para a verificação de versões de programas, bibliotecas, etc. que estão utilizando no site, junto da potencialidade de descobrir alguma falha devido a alguma mensagem de erro ou comentários no código fonte

Um exemplo pode ser pesquisar um diretório inexistente na página, isso se o site estiver mal configurado pode nos dar o banner do serviço de hospedagem, como no exemplo:
![[Pasted image 20250218154450.png]]


## Pesquisa via requisições HTTP

Podemos realizar a pesquisa via as requisições de HTTP via Netcat como visto anteriormente no curso da seguinte maneira:
```shell
nc -v rh.buisinesscorp.com.br 80
#Após isso conectou no servidor na porta HTPP

HEAD / HTTP/1.0
#Enviando essa requisição você irá receber o cabeçalho do site, mostrando
#a versão de diversas ferramentas, como hospedagem, sistema operacional, etc
#Resposta:
HTTP/1.1 200 OK
Date: Wed, 20 Sep 2017 02:49:36 GMT
Server: Apache/2.4.7 (Ubuntu)
X-Powered-By: PHP/5.5.9-1ubuntu4.22
Connection: close
Content-Type: text/html

#Podemos também descobrir os métodos suportados dentro do site
OPTIONS /desec HTTP/1.0

#Resposta:

HTTP/1.1 200 OK
Date: Wed, 20 Sep 2017 02:52:05 GMT
Server: Apache/2.4.7 (Ubuntu)
Allow: GET,HEAD,POST,OPTIONS
Content-Length: 0
Connection: close

```

*Dica: Caso queira fazer o teste em um servidor que hospede diversos sites você logo abaixo do cabeçalho pode enviar "Host: (host que deseja) para especificar e evitar erros"*

## Encontrando arquivos e diretórios

Existem inúmeras ferramentas que podem fazer a busca de arquivos e diretórios, alguns exemplos são: ``` dirb ```,```gobuster```,```dirsearch```.

Porém algumas das ferramentas podem ter algumas falhas e podem não identificar os diretórios que o site possui, por isso também é importante criar scripts própios para poder se esquivar de métodos de segurança do site.